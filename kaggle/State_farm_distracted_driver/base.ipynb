{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# state farm distracted driver detection\n",
    "base code : https://www.kaggle.com/zfturbo/keras-sample\n",
    "\n",
    "In this competition you are given driver images, each taken in a car with a driver doing something in the car (texting, eating, talking on the phone, makeup, reaching behind, etc). Your goal is to predict the likelihood of what the driver is doing in each picture.   \n",
    "\n",
    "![](https://kaggle2.blob.core.windows.net/competitions/kaggle/5048/media/output_DEb8oT.gif)\n",
    "\n",
    "The 10 classes to predict are:\n",
    "\n",
    "c0: safe driving  \n",
    "c1: texting - right  \n",
    "c2: talking on the phone - right  \n",
    "c3: texting - left  \n",
    "c4: talking on the phone - left  \n",
    "c5: operating the radio  \n",
    "c6: drinking  \n",
    "c7: reaching behind  \n",
    "c8: hair and makeup  \n",
    "c9: talking to passenger  \n",
    "\n",
    "\n",
    "To ensure that this is a computer vision problem, we have removed metadata such as creation dates. The train and test data are split on the drivers, such that one driver can only appear on either train or test set. \n",
    "\n",
    "To discourage hand labeling, we have supplemented the test dataset with some images that are resized. These processed images are ignored and don't count towards your score.\n",
    "\n",
    "Disclaimer: State Farm set up these experiments in a controlled environment - a truck dragging the car around on the streets - so these \"drivers\" weren't really driving. \n",
    "\n",
    "File descriptions\n",
    "\n",
    "imgs.zip - zipped folder of all (train/test) images  \n",
    "sample_submission.csv - a sample submission file in the correct format  \n",
    "driver_imgs_list.csv - a list of training images, their subject (driver) id, and class id  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import math\n",
    "import pickle\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(2016)\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.models import model_from_json\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy.misc import imread, imresize\n",
    "\n",
    "use_cache = 1\n",
    "color_type_global = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# color_type = 1 - gray\n",
    "# color_type = 3 - RGB\n",
    "\n",
    "def get_im_cv2(path, img_rows, img_cols, color_type=1):\n",
    "    # Load as grayscale\n",
    "    if color_type == 1:\n",
    "        img = cv2.imread(path, 0)\n",
    "    elif color_type == 3:\n",
    "        img = cv2.imread(path)\n",
    "    # Reduce size\n",
    "    resized = cv2.resize(img, (img_cols, img_rows))\n",
    "    return resized\n",
    "\n",
    "def get_im_cv2_mod(path, img_rows, img_cols, color_type=1):\n",
    "    # Load as grayscale\n",
    "    if color_type == 1:\n",
    "        img = cv2.imread(path, 0)\n",
    "    else:\n",
    "        img = cv2.imread(path)\n",
    "    # Reduce size\n",
    "    rotate = random.uniform(-10, 10)\n",
    "    M = cv2.getRotationMatrix2D((img.shape[1] / 2, img.shape[0] / 2), rotate,\n",
    "                                1)\n",
    "    img = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]))\n",
    "    resized = cv2.resize(img, (img_cols, img_rows), cv2.INTER_LINEAR)\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_driver_data():\n",
    "    dr = dict()\n",
    "    path = os.path.join('input', 'driver_imgs_list.csv')\n",
    "    print('Read drivers data')\n",
    "    f = open(path, 'r')\n",
    "    line = f.readline()\n",
    "    while (1):\n",
    "        line = f.readline()\n",
    "        if line == '':\n",
    "            break\n",
    "        arr = line.strip().split(',')\n",
    "        dr[arr[2]] = arr[0]\n",
    "    f.close()\n",
    "    return dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_train(img_rows, img_cols, color_type=1):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    driver_id = []\n",
    "    start_time = time.time()\n",
    "    driver_data = get_driver_data()\n",
    "\n",
    "    print('Read train images')\n",
    "    for j in range(10):\n",
    "        print('Load folder c{}'.format(j))\n",
    "        path = os.path.join('input', 'train', 'c' + str(j), '*.jpg')\n",
    "        files = glob.glob(path)\n",
    "        for fl in files:\n",
    "            flbase = os.path.basename(fl)\n",
    "            img = get_im_cv2_mod(fl, img_rows, img_cols, color_type)\n",
    "            X_train.append(img)\n",
    "            y_train.append(j)\n",
    "            driver_id.append(driver_data[flbase])\n",
    "\n",
    "    print('Read train data time: {} seconds'.format(\n",
    "        round(time.time() - start_time, 2)))\n",
    "    unique_drivers = sorted(list(set(driver_id)))\n",
    "    print('Unique drivers: {}'.format(len(unique_drivers)))\n",
    "    print(unique_drivers)\n",
    "    return X_train, y_train, driver_id, unique_drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_test(img_rows, img_cols, color_type=1):\n",
    "    print('Read test images')\n",
    "    start_time = time.time()\n",
    "    path = os.path.join('input', 'test', '*.jpg')\n",
    "    files = glob.glob(path)\n",
    "    X_test = []\n",
    "    X_test_id = []\n",
    "    total = 0\n",
    "    thr = math.floor(len(files) / 10)\n",
    "    for fl in files:\n",
    "        flbase = os.path.basename(fl)\n",
    "        img = get_im_cv2_mod(fl, img_rows, img_cols, color_type)\n",
    "        X_test.append(img)\n",
    "        X_test_id.append(flbase)\n",
    "        total += 1\n",
    "        if total % thr == 0:\n",
    "            print('Read {} images from {}'.format(total, len(files)))\n",
    "\n",
    "    print('Read test data time: {} seconds'.format(\n",
    "        round(time.time() - start_time, 2)))\n",
    "    return X_test, X_test_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cache_data(data, path):\n",
    "    if os.path.isdir(os.path.dirname(path)):\n",
    "        file = open(path, 'wb')\n",
    "        pickle.dump(data, file)\n",
    "        file.close()\n",
    "    else:\n",
    "        print('Directory doesnt exists')\n",
    "\n",
    "def restore_data(path):\n",
    "    data = dict()\n",
    "    if os.path.isfile(path):\n",
    "        file = open(path, 'rb')\n",
    "        data = pickle.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_model(model):\n",
    "    json_string = model.to_json()\n",
    "    if not os.path.isdir('cache'):\n",
    "        os.mkdir('cache')\n",
    "    open(os.path.join('cache', 'architecture.json'), 'w').write(json_string)\n",
    "    model.save_weights(\n",
    "        os.path.join('cache', 'model_weights.h5'), overwrite=True)\n",
    "    \n",
    "def read_model():\n",
    "    model = model_from_json(\n",
    "        open(os.path.join('cache', 'architecture.json')).read())\n",
    "    model.load_weights(os.path.join('cache', 'model_weights.h5'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_validation_set(train, target, test_size):\n",
    "    random_state = 51\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        train, target, test_size=test_size, random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_submission(predictions, test_id, info):\n",
    "    result1 = pd.DataFrame(\n",
    "        predictions,\n",
    "        columns=['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9'])\n",
    "    result1.loc[:, 'img'] = pd.Series(test_id, index=result1.index)\n",
    "    now = datetime.datetime.now()\n",
    "    if not os.path.isdir('subm'):\n",
    "        os.mkdir('subm')\n",
    "    suffix = info + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\"))\n",
    "    sub_file = os.path.join('subm', 'submission_' + suffix + '.csv')\n",
    "    result1.to_csv(sub_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_and_normalize_train_data(img_rows, img_cols, color_type=1):\n",
    "    cache_path = os.path.join('cache', 'train_r_' + str(img_rows) + '_c_' +\n",
    "                              str(img_cols) + '_t_' + str(color_type) + '.dat')\n",
    "    if not os.path.isfile(cache_path) or use_cache == 0:\n",
    "        train_data, train_target, driver_id, unique_drivers = load_train(\n",
    "            img_rows, img_cols, color_type)\n",
    "        cache_data((train_data, train_target, driver_id, unique_drivers),\n",
    "                   cache_path)\n",
    "    else:\n",
    "        print('Restore train from cache!')\n",
    "        (train_data, train_target, driver_id,\n",
    "         unique_drivers) = restore_data(cache_path)\n",
    "\n",
    "    train_data = np.array(train_data, dtype=np.uint8)\n",
    "    train_target = np.array(train_target, dtype=np.uint8)\n",
    "    train_data = train_data.reshape(train_data.shape[0], img_rows, img_cols, color_type)\n",
    "    train_target = np_utils.to_categorical(train_target, 10)\n",
    "    train_data = train_data.astype('float32')\n",
    "    train_data /= 255\n",
    "    print('Train shape:', train_data.shape)\n",
    "    print(train_data.shape[0], 'train samples')\n",
    "    return train_data, train_target, driver_id, unique_drivers\n",
    "\n",
    "def read_and_normalize_test_data(img_rows, img_cols, color_type=1):\n",
    "    cache_path = os.path.join('cache', 'test_r_' + str(img_rows) + '_c_' +\n",
    "                              str(img_cols) + '_t_' + str(color_type) + '.dat')\n",
    "    if not os.path.isfile(cache_path) or use_cache == 0:\n",
    "        test_data, test_id = load_test(img_rows, img_cols, color_type)\n",
    "        cache_data((test_data, test_id), cache_path)\n",
    "    else:\n",
    "        print('Restore test from cache!')\n",
    "        (test_data, test_id) = restore_data(cache_path)\n",
    "\n",
    "    test_data = np.array(test_data, dtype=np.uint8)\n",
    "    test_data = test_data.reshape(test_data.shape[0], img_rows, img_cols, color_type)\n",
    "    test_data = test_data.astype('float32')\n",
    "    test_data /= 255\n",
    "    print('Test shape:', test_data.shape)\n",
    "    print(test_data.shape[0], 'test samples')\n",
    "    return test_data, test_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dict_to_list(d):\n",
    "    ret = []\n",
    "    for i in d.items():\n",
    "        ret.append(i[1])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_several_folds_mean(data, nfolds):\n",
    "    a = np.array(data[0])\n",
    "    for i in range(1, nfolds):\n",
    "        a += np.array(data[i])\n",
    "    a /= nfolds\n",
    "    return a.tolist()\n",
    "\n",
    "\n",
    "def merge_several_folds_geom(data, nfolds):\n",
    "    a = np.array(data[0])\n",
    "    for i in range(1, nfolds):\n",
    "        a *= np.array(data[i])\n",
    "    a = np.power(a, 1 / nfolds)\n",
    "    return a.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def copy_selected_drivers(train_data, train_target, driver_id, driver_list):\n",
    "    data = []\n",
    "    target = []\n",
    "    index = []\n",
    "    for i in range(len(driver_id)):\n",
    "        if driver_id[i] in driver_list:\n",
    "            data.append(train_data[i])\n",
    "            target.append(train_target[i])\n",
    "            index.append(i)\n",
    "    data = np.array(data, dtype=np.float32)\n",
    "    target = np.array(target, dtype=np.float32)\n",
    "    index = np.array(index, dtype=np.uint32)\n",
    "    return data, target, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model_v1(img_rows, img_cols, color_type=1):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, 3, 3, border_mode='same', init='he_normal',\n",
    "              input_shape=(img_rows, img_cols, color_type)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Convolution2D(64, 3, 3, border_mode='same', init='he_normal'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Convolution2D(128, 3, 3, border_mode='same', init='he_normal'))\n",
    "    model.add(MaxPooling2D(pool_size=(8, 8)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(Adam(lr=1e-3), loss='categorical_crossentropy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_single():\n",
    "    # input image dimensions\n",
    "    img_rows, img_cols = 64, 64\n",
    "    batch_size = 32\n",
    "    nb_epoch = 10\n",
    "\n",
    "    train_data, train_target, driver_id, unique_drivers = read_and_normalize_train_data(\n",
    "        img_rows, img_cols, color_type_global)\n",
    "    test_data, test_id = read_and_normalize_test_data(img_rows, img_cols,\n",
    "                                                      color_type_global)\n",
    "\n",
    "    yfull_train = dict()\n",
    "    yfull_test = []\n",
    "    unique_list_train = [\n",
    "        'p002', 'p012', 'p014', 'p015', 'p016', 'p021', 'p022', 'p024', 'p026',\n",
    "        'p035', 'p039', 'p041', 'p042', 'p045', 'p047', 'p049', 'p050', 'p051',\n",
    "        'p052', 'p056', 'p061', 'p064', 'p066', 'p072', 'p075'\n",
    "    ]\n",
    "    X_train, Y_train, train_index = copy_selected_drivers(\n",
    "        train_data, train_target, driver_id, unique_list_train)\n",
    "    unique_list_valid = ['p081']\n",
    "    X_valid, Y_valid, test_index = copy_selected_drivers(\n",
    "        train_data, train_target, driver_id, unique_list_valid)\n",
    "\n",
    "    print('Start Single Run')\n",
    "    print('Split train: ', len(X_train), len(Y_train))\n",
    "    print('Split valid: ', len(X_valid), len(Y_valid))\n",
    "    print('Train drivers: ', unique_list_train)\n",
    "    print('Test drivers: ', unique_list_valid)\n",
    "\n",
    "    model = create_model_v1(img_rows, img_cols, color_type_global)\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        Y_train,\n",
    "        batch_size=batch_size,\n",
    "        nb_epoch=nb_epoch,\n",
    "        show_accuracy=True,\n",
    "        verbose=1,\n",
    "        validation_data=(X_valid, Y_valid))\n",
    "\n",
    "    # score = model.evaluate(X_valid, Y_valid, show_accuracy=True, verbose=0)\n",
    "    # print('Score log_loss: ', score[0])\n",
    "\n",
    "    predictions_valid = model.predict(X_valid, batch_size=128, verbose=1)\n",
    "    score = log_loss(Y_valid, predictions_valid)\n",
    "    print('Score log_loss: ', score)\n",
    "\n",
    "    # Store valid predictions\n",
    "    for i in range(len(test_index)):\n",
    "        yfull_train[test_index[i]] = predictions_valid[i]\n",
    "\n",
    "    # Store test predictions\n",
    "    test_prediction = model.predict(test_data, batch_size=128, verbose=1)\n",
    "    yfull_test.append(test_prediction)\n",
    "\n",
    "    print('Final log_loss: {}, rows: {} cols: {} epoch: {}'.format(\n",
    "        score, img_rows, img_cols, nb_epoch))\n",
    "    info_string = 'loss_' + str(score) \\\n",
    "                    + '_r_' + str(img_rows) \\\n",
    "                    + '_c_' + str(img_cols) \\\n",
    "                    + '_ep_' + str(nb_epoch)\n",
    "\n",
    "    test_res = merge_several_folds_mean(yfull_test, 1)\n",
    "    create_submission(test_res, test_id, info_string)\n",
    "\n",
    "\n",
    "run_single()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
